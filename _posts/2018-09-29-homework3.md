---
title: "Melody"
---
#### Generative Sequences

# TODO
* Create Particle Class
* Create Personalities that move across the screen
* Map Particle Class to Grid
* Create Tone
* Create Rhythm
* Create Particle Animations after notes are played

# Idea
I came up with this week's idea pretty quickly and did not deviate from it much.

Furthermore, I planned pretty quickly to use the Ryukyu scale, a Japanese collection containing the notes (Do, Mi, Fa, Sol, Ti, Do). Back in youth, when I was learning about synthesis, I got a video game for my Nintendo DS called [Korg DS-10](https://en.wikipedia.org/wiki/KORG_DS-10).

# Implementation
This sketch is a bit more complicated than last week's, but because I overcame numerous hurdles last week (e.g. sequencing, 2d list processing), it all felt manageable. The big challenge in working with p5.js and Tone.js is managing big structures that control visuals and audio. These two things are related, but are called differently at different times. (I wonder if anyone has come up with an ideal method of arranging their code with these two libraries?) In this week's solution, I created a `Particle` class which has a static variable `Particle.positions`, a 2d array of 0's and 1's that are fed into the `Tone.Sequence` and updated as particles get added and move around the screen. This is useful because the audio is directly drawn from the positions of objects on screen and both code written in `particle.js` and `sketch.js` have access to this variable. I also created a `Grid` class for animating the step sequencer grid, but I found it useful in otherways too: Its `get_position()` function returns pixel coordinates based on grid position, and its `get_radius()` function can used for animating particles.

# Result
I think that this came out pretty well and is my most musically satisfying project yet. Because particles die out, the user has to press buttons really fast if they want the music to build. The instrument does not just play on its own but requires constant input like any real instrument. And, because particles lose amplitude as they die, new particles function as accented notes. Particles are added with a skewed random function meaning they tend to have a lower pitch, giving higher pitches more weight, and I put a bit of effort in getting the reverb, panning, and amplitude just right.

One potential issue is the rate at which things change - notes die pretty fast and can sometimes move around a lot. This can make it hard to lock into an enjoyable pattern one happens into. If this were to emulate, say a Steve Reich piece, then notes should hardly move (like once every couple measures). But, on the internet our attention spans are much lower, so maybe this is good given the circumstances.

# Future Ideas
Depending on how the next few projects go, I may be interested in upgrading this one for the midterm.
* Because of the way I have loaded in sounds, it would not be too hard to change scales in real time. Maybe, once the user has reached
* It would be awesome if the background changed as the piece progresses. Maybe there is some way to represent on the grid how many particles have died at each point?
* I know I've been into single button inputs, but a low-hanging fruit would be to add mouse click support for specific notes. I could also add a few sliders around the interface to control some of the features, like tempo, particle lifespan, and chance for movement.

* [Play it. (Only supported in Chrome.)](https://huriphoonado.github.io/code-of-music/projects/Melody)
* [View Source in Github.](https://github.com/Huriphoonado/code-of-music/tree/master/projects/Melody)
